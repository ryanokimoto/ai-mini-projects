{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "xLLDljNCNzeI",
        "notebookgrader": {
          "id": "a6839cdc0a8d4ab8b5d25cb5853c26cb8d699ee0",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import collections\n",
        "import math\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pc_eaodEtshM",
        "notebookgrader": {
          "id": "0affc217b7b135198e3fee2e21d3fd34e35e500d",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8f45ef0a-c66a-4537-df84-993309fbe745"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'requests' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-135048697.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mall_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[a-zA-Z]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbook_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
          ]
        }
      ],
      "source": [
        "# Computes the global vocabulary.\n",
        "urls = [\n",
        "    \"https://storage.googleapis.com/research-share/texts/t10.txt\",\n",
        "    \"https://storage.googleapis.com/research-share/texts/t1.txt\",\n",
        "    \"https://storage.googleapis.com/research-share/texts/t2.txt\",\n",
        "    \"https://storage.googleapis.com/research-share/texts/t3.txt\",\n",
        "    \"https://storage.googleapis.com/research-share/texts/t4.txt\",\n",
        "    \"https://storage.googleapis.com/research-share/texts/t5.txt\",\n",
        "    \"https://storage.googleapis.com/research-share/texts/t6.txt\",\n",
        "    \"https://storage.googleapis.com/research-share/texts/t7.txt\",\n",
        "    \"https://storage.googleapis.com/research-share/texts/t8.txt\",\n",
        "    \"https://storage.googleapis.com/research-share/texts/t9.txt\",\n",
        "]\n",
        "\n",
        "# We compute the book tokens once and forall, as this is an expensive operation.\n",
        "book_tokens = []\n",
        "all_tokens = []\n",
        "for u in urls:\n",
        "    text = requests.get(u).text\n",
        "    tokens = re.findall(r'[a-zA-Z]+', text)\n",
        "    book_tokens.append(tokens)\n",
        "    all_tokens.extend(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpRzmLHBmipp",
        "notebookgrader": {
          "id": "02281031551f16537e645bacce3088e375a1399f",
          "readonly": true
        }
      },
      "source": [
        "We remind you of the main formulas.\n",
        "\n",
        "For a word $w$, let:\n",
        "* $P(w)$ be the probability of word $w$ in the general vocabulary.\n",
        "* $K_w$ be the number of times the author uses $w$ in the author's text.\n",
        "* $K$ be the number of words in the author's text.\n",
        "* $\\alpha$ be the Laplace smooting coefficient.\n",
        "\n",
        "Then, the probability that the author generates word $w$ is given by:\n",
        "\n",
        "$$\\theta_w = \\frac{K_w + \\alpha P(w)}{K + \\alpha}$$\n",
        "\n",
        "Let $M_w$ the number of times word $w$ appears in an unknown body of text, and let $M = \\sum_w M_w$.\n",
        "The log-likelyhood of the author generating the unknown body of text is then:\n",
        "\n",
        "$$\\sum_w M_w \\log \\theta_w$$\n",
        "\n",
        "With these formulas, you should be able to complete the class below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS0qEmR9keIS",
        "notebookgrader": {
          "id": "3d409fd44853662789557479ee514d8d96f2a7f1",
          "is_solution": true,
          "is_tests": false,
          "readonly": false
        }
      },
      "outputs": [],
      "source": [
        "class Author(object):\n",
        "\n",
        "    def __init__(self, name, tokens, alpha=100, vocabulary_probabilities=None):\n",
        "        \"\"\"Initializes the author with the given name and text.\n",
        "        Also tokenizes the text with spaCy, and stores the list of tokens.\n",
        "        @param name: name of the author.\n",
        "        @param text: tokenized text constituting writing sample for the author.\n",
        "        @param alpha: value to be used in the Laplace smoothing function.\n",
        "        @param vocabulary_probabilities: dictionary mapping each word to its probability\n",
        "            in the vocabulary (general vocabulary, not by the author).\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.text = text\n",
        "        self.alpha = alpha\n",
        "        self.tokens = tokens\n",
        "        self.num_tokens = len(self.tokens)\n",
        "        # We also build for you a dictionary mapping each word to its number of occurrences in the\n",
        "        self.word_frequencies = {w: c for w, c in collections.Counter(self.tokens).items()}\n",
        "        # We store the underlying word probabilities in the dictionary.\n",
        "        self.vocabulary_word_probabilities = vocabulary_probabilities if vocabulary_probabilities is not None else {}\n",
        "\n",
        "\n",
        "    def theta(self, word):\n",
        "        \"\"\"Returns the probability that an author generates a given word.\n",
        "        This is the theta in the above mathematical formula.\n",
        "        \"\"\"\n",
        "        kw = self.word_frequencies[word] if word in self.word_frequencies else 0\n",
        "        pw = self.vocabulary_word_probabilities[word] if word in self.vocabulary_word_probabilities else 0\n",
        "        return (kw + self.alpha * pw) / (self.num_tokens + self.alpha)\n",
        "\n",
        "    def author_likelyhood(self, token_list):\n",
        "        \"\"\"Returns the log-likelyhood of the author having written the given text.\n",
        "        The given text is given purely as a sequence of tokens.\n",
        "        \"\"\"\n",
        "        total = 0\n",
        "        given_text_word_frequencies = {w: c for w, c in collections.Counter(token_list).items()}\n",
        "        for w in given_text_word_frequencies:\n",
        "          total += (given_text_word_frequencies[w] * math.log(self.theta(w)))\n",
        "        return total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pja5xuQWCL3P",
        "notebookgrader": {
          "id": "1d4004546d85f83d617adf442eb7e233d673a7b4",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        }
      },
      "outputs": [],
      "source": [
        "c = collections.Counter(all_tokens)\n",
        "num_tokens = len(all_tokens)\n",
        "probs = {word: count / num_tokens for word, count in c.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVtrselRqI-3",
        "notebookgrader": {
          "id": "1145eecc2f851c39c745ad89419790a7ca5f8394",
          "readonly": true
        }
      },
      "source": [
        "Ok.  Now, we can create our three authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjwpnJC-fYsK",
        "notebookgrader": {
          "id": "6a4044f579a540cc8fbde8b59290f3b6ef6602ec",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        }
      },
      "outputs": [],
      "source": [
        "true_books = {\n",
        "    'doyle': book_tokens[1],\n",
        "    'austin': book_tokens[2],\n",
        "    'christie': book_tokens[3],\n",
        "}\n",
        "\n",
        "# We can build authors from these writing samples.\n",
        "authors = {\n",
        "    name: Author(name, tokens, vocabulary_probabilities=probs) for name, tokens in true_books.items()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMU_PgdaAHWh",
        "notebookgrader": {
          "id": "ac3fc678f9b73fadbdf6c6664cb6f0119d1e6ab7",
          "readonly": true
        }
      },
      "source": [
        "Now, given a piece of text, let us write a function that returns the most likely author."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UONZDk1ApVl",
        "notebookgrader": {
          "id": "4279a57295c15a720a5ccd79499a06c83bf13466",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        }
      },
      "outputs": [],
      "source": [
        "def most_likely_author(authors, tokens):\n",
        "    return max(authors, key=lambda a: authors[a].author_likelyhood(tokens))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVZDmfQFA1Ot",
        "notebookgrader": {
          "id": "a19110b5d9ead1e7934379162a2d383a40a0c4fa",
          "readonly": true
        }
      },
      "source": [
        "Let us check that it works for the authors from which it has been trained -- it has better to!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH6vTlgVsonJ",
        "notebookgrader": {
          "id": "0fb4b84ab7babbd359b706aeb1183ce850482ee9",
          "is_solution": false,
          "is_tests": true,
          "readonly": true,
          "test_points": 10
        },
        "outputId": "eeac7062-ed16-426f-aed3-635c04a932b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doyle is attributed to doyle\n",
            "austin is attributed to austin\n",
            "christie is attributed to christie\n"
          ]
        }
      ],
      "source": [
        "# Tests 10 points: Training set is correctly attributed.\n",
        "for name, tokens in true_books.items():\n",
        "    most_likely = most_likely_author(authors, tokens)\n",
        "    print(f\"{name} is attributed to {most_likely}\")\n",
        "    assert name == most_likely\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHaH0q1lDio2",
        "notebookgrader": {
          "id": "362df89041048019d2e3b5c898472f93ed8bcd3b",
          "readonly": true
        }
      },
      "source": [
        "Now, let's check the other attributions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m0xdK7kStNl",
        "notebookgrader": {
          "id": "3bc612dc1986dfc21f94f96c063a9581ca64cf37",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        },
        "outputId": "80f3a471-c919-47a1-d4de-1fb7ce2e6199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "austin\n",
            "doyle\n",
            "austin\n",
            "christie\n",
            "doyle\n",
            "doyle\n",
            "doyle\n",
            "doyle\n",
            "doyle\n",
            "christie\n"
          ]
        }
      ],
      "source": [
        "for n in range(0, 10):\n",
        "    print(most_likely_author(authors, book_tokens[n]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L60pgOz-WV-F",
        "notebookgrader": {
          "id": "f374007a918da91a3e39f46f87d121fd89c9d3a8",
          "readonly": true
        }
      },
      "source": [
        "Uhm, this is... ok... but: there are some books above that are not written by either of Doyle, Austin, or Christie.  We want a version of `most_likely_author` that can return a tuple, consisting of the best author, its log-likelihood, and the difference in log-likelyhood between first and second attribution.  \n",
        "I let you write it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQjKIty5WpDO",
        "notebookgrader": {
          "id": "13673613d421d1ea83d405c6e6e744a65e71bb0e",
          "is_solution": true,
          "is_tests": false,
          "readonly": false
        }
      },
      "outputs": [],
      "source": [
        "def attribution(authors, tokens):\n",
        "    \"\"\"Returns a tuple consisting of:\n",
        "    * name of the best author\n",
        "    * log-likelyhood of the best author.\n",
        "    * the difference in log-likelyhood between first and second most likely authors.\n",
        "    \"\"\"\n",
        "    # Just to avoid everything breaking if you leave the placeholder blank.\n",
        "    author = most_likely_author(authors, tokens)\n",
        "\n",
        "    authors_map = {}\n",
        "    for a in authors:\n",
        "      author = authors[a]\n",
        "      authors_map[a] = author.author_likelyhood(tokens)\n",
        "\n",
        "    author2 = sorted(authors_map.items())[-2][0]\n",
        "\n",
        "    result = (\n",
        "        most_likely_author(authors, tokens),\n",
        "        authors_map[most_likely_author(authors, tokens)],\n",
        "        authors_map[most_likely_author(authors, tokens)] - authors_map[author2]\n",
        "    )\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1phBKr0bDWP",
        "notebookgrader": {
          "id": "dd79381b7896758dea6cb2c2e2e7454716cba1cd",
          "readonly": true
        }
      },
      "source": [
        "Here is a test to help you check your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BY6TmX9bCbQ",
        "notebookgrader": {
          "id": "9f496c9ea855a707558cf1508f9c615586d825dd",
          "is_solution": false,
          "is_tests": true,
          "readonly": true,
          "test_points": 10
        },
        "outputId": "91c88e43-409a-4105-9336-2ce501695fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "austin -62545.83572405311 6490.46375791086\n"
          ]
        }
      ],
      "source": [
        "# Tests 10 points: attribution\n",
        "\n",
        "a, l, d = attribution(authors, book_tokens[0])\n",
        "print(a, l, d)\n",
        "assert a == \"austin\"\n",
        "assert -63000 < l < -62000\n",
        "assert 6400 < d < 6500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dDED7AYbgSH",
        "notebookgrader": {
          "id": "cd9f786965c67be5fb815f06099ffd8e35043b84",
          "readonly": true
        }
      },
      "source": [
        "We can now print some attributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bas0HacNX0dq",
        "notebookgrader": {
          "id": "090e3f08dc8a83d0ec5f6dca02c2ea3b8524cfdd",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88a274b0-e907-4a9b-e6ad-08e5a2bb2568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 0: Attributed to austin, likelyhood -62545.83572405311, difference from next likely: 6490.46375791086\n",
            "Text 1: Attributed to doyle, likelyhood -51049.5990972075, difference from next likely: 12372.111709314457\n",
            "Text 2: Attributed to austin, likelyhood -54683.66775402256, difference from next likely: 14748.195974026072\n",
            "Text 3: Attributed to christie, likelyhood -55587.56025347501, difference from next likely: 0.0\n",
            "Text 4: Attributed to doyle, likelyhood -70292.65086164579, difference from next likely: 1078.1875740217802\n",
            "Text 5: Attributed to doyle, likelyhood -75462.0292245311, difference from next likely: 98.37916958754067\n",
            "Text 6: Attributed to doyle, likelyhood -63349.268280002, difference from next likely: 2726.5613402772287\n",
            "Text 7: Attributed to doyle, likelyhood -72188.7588726332, difference from next likely: 844.6176847602474\n",
            "Text 8: Attributed to doyle, likelyhood -76350.13362141213, difference from next likely: 470.24902889663645\n",
            "Text 9: Attributed to christie, likelyhood -64835.74715507077, difference from next likely: 0.0\n"
          ]
        }
      ],
      "source": [
        "for n in range(0, 10):\n",
        "    a, l, d = attribution(authors, book_tokens[n])\n",
        "    print(f\"Text {n}: Attributed to {a}, likelyhood {l}, difference from next likely: {d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsW0Li0pbr5U",
        "notebookgrader": {
          "id": "8fa4bc4107097b2eaca9d8a465837381f4f11926",
          "readonly": true
        }
      },
      "source": [
        "Now, this is fine, but we can try to improve this result.  One hypothesis is that rare words, which are used in only portions of the texts by an author, can sway the results.  It might be a better idea, to produce a \"fingerprint\" of an author, to consider the frequency of relatively frequent words only.\n",
        "For example, we can discard both the most frequent, and the least frequent, words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXEwaUBWcFYh",
        "notebookgrader": {
          "id": "5aee8d7d5506e9ceee3371afe08bfa5da316d4a2",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        }
      },
      "outputs": [],
      "source": [
        "LOW_THRESHOLD = 50\n",
        "HIGH_THRESHOLD = 2000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWcpaaEscRyo",
        "notebookgrader": {
          "id": "63b88cc674e0c55e0756026b28c52534438ff27e",
          "readonly": true
        }
      },
      "source": [
        "We can then filter only the tokens that are frequent enough, and redo our author model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hd2MHXISa0M",
        "notebookgrader": {
          "id": "71d0cdf479cb6beb7407ec60bea3384adfb8c1e8",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        }
      },
      "outputs": [],
      "source": [
        "c = collections.Counter(all_tokens)\n",
        "most_common = [t for t, _ in c.most_common(HIGH_THRESHOLD)[LOW_THRESHOLD:]]\n",
        "\n",
        "def filter_tokens(tokens):\n",
        "    return [t for t in tokens if t in most_common]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq8jov6cTx8C",
        "notebookgrader": {
          "id": "c0d36b0530928d951b77d64ec852b92b7ee5e8a4",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        }
      },
      "outputs": [],
      "source": [
        "new_book_tokens = list(map(filter_tokens, book_tokens))\n",
        "new_all_tokens = list(map(filter_tokens, all_tokens))\n",
        "new_num_tokens = len(new_all_tokens)\n",
        "c = collections.Counter(all_tokens)\n",
        "new_probs = {word: count / new_num_tokens for word, count in c.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zALwHngVT2R-",
        "notebookgrader": {
          "id": "9f7ed2e38abe778b653a2106537a5a611a8d3f8f",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        }
      },
      "outputs": [],
      "source": [
        "new_true_books = {\n",
        "    'doyle': new_book_tokens[1],\n",
        "    'austin': new_book_tokens[2],\n",
        "    'christie': new_book_tokens[3],\n",
        "}\n",
        "\n",
        "# We can build authors from these writing samples.\n",
        "new_authors = {\n",
        "    name: Author(name, tokens, alpha=100, vocabulary_probabilities=new_probs) for name, tokens in new_true_books.items()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl4VoO_geeOu",
        "notebookgrader": {
          "id": "4c47e5e38db762f10001eb4f71d3408342b10096",
          "readonly": true
        }
      },
      "source": [
        "Let's look at the attributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-CoKFLKU22g",
        "notebookgrader": {
          "id": "df298a118b5e321ed4a97cb11b224c4bfac4db12",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9bfc1b6-80c9-4431-810d-7b9ffec8188d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 0: Attributed to austin, likelyhood -30567.859752043467, difference from next likely: 5480.522747214603\n",
            "Text 1: Attributed to doyle, likelyhood -25072.632555619934, difference from next likely: 6850.968481753829\n",
            "Text 2: Attributed to austin, likelyhood -27354.669893484013, difference from next likely: 9205.041184312522\n",
            "Text 3: Attributed to christie, likelyhood -28518.835141421245, difference from next likely: 0.0\n",
            "Text 4: Attributed to doyle, likelyhood -31739.92316895656, difference from next likely: 973.7540347390823\n",
            "Text 5: Attributed to doyle, likelyhood -31619.023436477302, difference from next likely: 12.950940240451018\n",
            "Text 6: Attributed to doyle, likelyhood -29387.587715004964, difference from next likely: 2357.2222331456687\n",
            "Text 7: Attributed to doyle, likelyhood -30123.4028306915, difference from next likely: 818.883843137417\n",
            "Text 8: Attributed to doyle, likelyhood -30777.66571182321, difference from next likely: 160.76767802321046\n",
            "Text 9: Attributed to christie, likelyhood -33754.15553342655, difference from next likely: 0.0\n"
          ]
        }
      ],
      "source": [
        "for n in range(0, 10):\n",
        "    a, l, d = attribution(new_authors, new_book_tokens[n])\n",
        "    print(f\"Text {n}: Attributed to {a}, likelyhood {l}, difference from next likely: {d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znG23elRdt8j",
        "notebookgrader": {
          "id": "23160e1d7314783eb330d40c9e84e769bd2cda32",
          "readonly": true
        }
      },
      "source": [
        "Now, there are two impostors in the list: two texts that are in fact not authored by any of Austin, Doyle, or Christie.  Can you tell which ones they are?  Hint: a measure of confidence is the difference between most likely attribution, and runner-up.\n",
        "\n",
        "Give a response with the form:\n",
        "\n",
        "    wrong_attributions = [6, 7]\n",
        "\n",
        "where the numbers refer to the texts above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5MRcQifVjU1",
        "notebookgrader": {
          "id": "bb7fb4284e8eecd0f1b5596e6f3e9d83f3192530",
          "is_solution": false,
          "is_tests": false,
          "readonly": true
        }
      },
      "outputs": [],
      "source": [
        "# Set the wrong_attributions variable to the wrong attributions.\n",
        "\n",
        "wrong_attributions = [3, 9]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_VW8XeHgI39",
        "notebookgrader": {
          "id": "1d91056dba93d0f74fe18c569d211ca75f996bf0",
          "is_solution": false,
          "is_tests": true,
          "readonly": true,
          "test_points": 10
        }
      },
      "outputs": [],
      "source": [
        "# Tests 10 points: wrong attributions.\n",
        "\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvxp0JWVC_bT",
        "notebookgrader": {
          "id": "915fa1d85b6bdd8442cf961649b558938ebaa6dc",
          "readonly": true
        }
      },
      "source": [
        "Does this work well?  So-so, right?  We will try another approach to authorship attribution in a future homework."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "notebookgrader": {
      "total_points": 30
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}