"""
Evaluation script for the bag-of-words classifier.

This module provides functionality to evaluate a trained model on test data
and compute various performance metrics including precision, recall, and F1 scores.

WARNING: This file is auto-generated or part of the autograder system.
DO NOT EDIT this file as changes will be overwritten.
"""

import argparse

import joblib
import numpy as np
import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import precision_recall_fscore_support

from model import get_best_model, predict
from preprocess import get_data


def evaluate_metrics(model, dataloader, predict_fn, device='cpu'):
    """
    Evaluate the model on the given dataloader and compute performance metrics.
    
    Args:
        model: The trained model to evaluate
        dataloader: DataLoader containing test data
        predict_fn: A function to convert model outputs to final Boolean predictions
        device: Device to run evaluation on (CPU or GPU) (default: CPU)
    
    Returns:
        Tuple containing:
            - Precision scores for each class
            - Recall scores for each class
            - F1 scores for each class
            - Weighted F1 score
            - Support (number of true instances) for each class
    """
    model.eval()
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            
            predictions = predict_fn(outputs)

            all_predictions.append(predictions.cpu().numpy())
            all_targets.append(targets.cpu().numpy())

    # Concatenate all batches
    all_predictions = np.vstack(all_predictions)
    all_targets = np.vstack(all_targets)
    
    # Calculate metrics for each class
    precision, recall, f1, support = precision_recall_fscore_support(
        all_targets, all_predictions, average=None, zero_division=0
    )
    
    # Get weighted averages
    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
        all_targets, all_predictions, average='weighted', zero_division=0
    )
    
    return f1_weighted, precision, recall, f1, support


def parse_arguments():
    """
    Parse command line arguments for model evaluation.
    
    Returns:
        Parsed command line arguments
    """
    parser = argparse.ArgumentParser(description="Evaluate a trained bag-of-words classifier model")
    parser.add_argument('--test_path', default='./data/test.csv', help='Path to the test dataset CSV file')
    parser.add_argument('--labels_path', default='./data/all_labels.csv', help='Path to the labels file')
    parser.add_argument('--vectorizer_path', default='./vectorizer.joblib', help='Path to the trained vectorizer file')
    parser.add_argument('--model_path', default='./best_model.pt', help='Path to the trained model file')
    parser.add_argument('--batch_size', type=int, default=64, help='Batch size for evaluation')
    
    return parser.parse_args()


def main():
    """Main function to run model evaluation."""
    args = parse_arguments()

    # Load possible labels
    with open(args.labels_path, 'r', encoding='utf-8') as f_labels:
        labels = [line.strip() for line in f_labels]
    label_to_id = {label: i for i, label in enumerate(labels)}

    # Load pretrained vectorizer (turns sentences to bag of words tensor)
    bow_vectorizer = joblib.load(args.vectorizer_path)
    
    # Load and prepare test data
    x_test, y_test = get_data(
        args.test_path, 
        bow_vectorizer, 
        include_y=True, 
        label_to_id=label_to_id
    )
    test_dataset = TensorDataset(x_test, y_test)  
    test_loader = DataLoader(
        test_dataset, 
        batch_size=args.batch_size, 
        shuffle=False
    )

    # Load trained model
    bow_model = get_best_model(input_size=len(bow_vectorizer.vocabulary_), num_labels=len(label_to_id))
    bow_model.load_state_dict(torch.load(args.model_path, map_location='cpu'))

    # Set up device and run evaluation
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    bow_model.to(device)
    
    predict_fn = predict
    f1_weighted, precision, recall, f1, support = evaluate_metrics(
        bow_model, test_loader, predict_fn, device
    )

    # Print results
    print(f"Weighted F1: {f1_weighted:.4f}")
    print("Per class performance:")
    for i in range(len(precision)):
        print(f"  Class {i:2d}: P={precision[i]:.3f}, R={recall[i]:.3f}, "
              f"F1={f1[i]:.3f}, Support={support[i]}")


if __name__ == "__main__":
    main()
    